{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T16:06:17.299690Z",
     "start_time": "2025-01-30T16:06:17.293952Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "import torchvision\n",
    "from torch import optim, nn\n",
    "import timeit"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T16:07:19.546349Z",
     "start_time": "2025-01-30T16:07:19.518825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 28\n",
    "IN_CHANNELS = 1\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.001\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "NUM_ENCODERS = 4\n",
    "EMBED_DIM = (PATCH_SIZE ** 2) * IN_CHANNELS # 16\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2 # 49\n",
    "\n",
    "# Expansion of the MLP\n",
    "EXPANSION = 128\n",
    "\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "356b7210c547114",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
    "      super().__init__()\n",
    "      self.patcher = nn.Sequential(\n",
    "          # We use conv for doing the patching\n",
    "          nn.Conv2d(\n",
    "              in_channels=in_channels,\n",
    "              out_channels=embed_dim,\n",
    "              # if kernel_size = stride -> no overlap\n",
    "              kernel_size=patch_size,\n",
    "              stride=patch_size\n",
    "          ),\n",
    "          # Linear projection of Flattened Patches. We keep the batch and the channels (b,c,h,w)\n",
    "          nn.Flatten(2))\n",
    "      self.cls_token = nn.Parameter(torch.randn(size=(1, 1, embed_dim)), requires_grad=True)\n",
    "      self.position_embeddings = nn.Parameter(torch.randn(size=(1, num_patches+1, embed_dim)), requires_grad=True)\n",
    "      self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "      # Create a copy of the cls token for each of the elements of the BATCH\n",
    "      cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "      # Create the patches\n",
    "      x = self.patcher(x).permute(0, 2, 1)\n",
    "      # Unify the position with the patches\n",
    "      x = torch.cat([cls_token, x], dim=1)\n",
    "      # Patch + Position Embedding\n",
    "      x = self.position_embeddings + x\n",
    "      x = self.dropout(x)\n",
    "      return x"
   ],
   "id": "acb643041a2dcc42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
